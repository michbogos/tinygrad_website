<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>tinygrad: A simple and powerful neural network framework</title>
    <style>
        body{
            display:grid;
            place-items: center;
            font-family:'Lucida Console', monospace;
            font-size: medium;
            row-gap: 1em;
            background: rgb(217,255,245);
            background: radial-gradient(circle, rgba(217,255,245,1) 0%, rgba(238,229,233,1) 35%, rgba(201,228,231,1) 100%); 
        }
        h1, h2, h3, h4, h5, h6{
            margin-top: 0;
            margin-bottom: 0;
        }

        p{
            min-width: 20ch;
        }

        div{
            width:min(80ch, 100vw);
        }

        li{
            margin-left: 1em;
        }

        .stack{
            display: flex;
            flex-direction: column;
            gap: 1em;
        }
    </style>
</head>
<body>
    <h1>We are the tiny corp</h1>
    <h2>What's the goal of the tiny corp?</h2>
    
    <h3>To accelerate. We will commoditize the petaflop and enable AI for everyone.</h3>

    <div>
        <p>We write and maintain <a href="https://github.com/tinygrad/tinygrad">tinygrad</a>, the <a href="https://star-history.com/#tinygrad/tinygrad&Date">fastest growing</a> neural network framework (over 22000 GitHub stars)<p>
        <p>It's extremely simple, and breaks down the most <a href="https://github.com/tinygrad/tinygrad/blob/master/examples/llama.py">complex</a> <a href="https://github.com/geohot/tinygrad/blob/master/examples/stable_diffusion.py">networks</a> into 4 <a href="https://github.com/geohot/tinygrad/blob/master/tinygrad/ops.py">OpTypes</a></p>
        <a href="https://github.com/tinygrad/tinygrad"><img src="logo.webp" style="float:right"/></a>
        <li><b>UnaryOps</b> operate on one tensor and run elementwise. RELU, LOG, RECIPROCAL, etc...</li>
        <li><b>BinaryOps</b> operate on two tensors and run elementwise to return one. ADD, MUL, etc...</li>
        <li><b>ReduceOps</b> operate on one tensor and return a smaller tensor. SUM, MAX</li>
        <li><b>MovementOps</b> operate on one tensor and move the data around, copy-free with <a href="https://github.com/tinygrad/tinygrad/blob/master/tinygrad/shape/shapetracker.py">ShapeTracker</a>. RESHAPE, PERMUTE, EXPAND, etc...</li>
        <p>But how...where are your CONVs and MATMULs? Read the code to solve this mystery.</p>
    </div>

    <div>
        <h2>Work at the tiny corp</h2>
        
        We <a href="https://geohot.github.io/blog/jekyll/update/2023/05/24/the-tiny-corp-raised-5M.html">are now funded</a> and <b>hiring</b> full time software engineers. Very talented interns okay.<br/>
        
        See <b>jobs that need doing</b> to judge if you might be a good fit.<br/><br/>
        
        I'm supposed to pitch you emotionally on why you should work here.<br/>
        It will give you that meaning you are searching for, and I'm only half kidding.<br/>
        We are actually trying to make AI for everyone.<br/><br/>
        
        <i>work@tinygrad.org</i> to apply, but really, get hired by <a href="https://docs.google.com/spreadsheets/d/1WKHbT-7KOgjEawq5h5Ic1qUWzpfAzuD_J06N1JwOCGs/edit?usp=sharing">doing bounties</a>.<br/>
        If you haven't contributed to tinygrad, your application won't be considered.

    </div>
    
    <div>
        <h2>Jobs that need doing</h2>
        
        If you come work here, this is a list to get started. Assumes some familiarity with the <a href="https://github.com/tinygrad/tinygrad">tinygrad</a> codebase.
    </div>    
        
    <div>
        <h3>Getting AMD on MLPerf</h3>
        There's a <a href="https://github.com/users/geohot/projects/2">GitHub project</a> for this. Can you make the models train?
        $500 per model to get tinygrad to train from scratch to the required error within 12 hours.
        Learn more in the #mlperf-bounties channel on our <a href="https://discord.gg/ZjZadyC7PK">Discord</a>
    </div>
        
    <div>
        <h3>Fastest Stable Diffusion and LLaMA on M1</h3>
        For LLaMA: Write the specializedjit and support int8.
        For Stable Diffusion: Clean up to use BS=2, write winograd conv and kernel search. Target is 0.324 s/it on M1 Max.<br/>
    </div>
    
    <div>
        <h3>Qualcomm DSP Support</h3>
        Switch over comma's DSP based driver monitoring model to tinygrad.
        A new architecture to port!
    </div>
    
    <div>
        <h3>Max out all the GEMMs</h3>
        NVIDIA: Support local memory tensors. Some work in <a href="https://github.com/tinygrad/tinygrad/pull/557">this branch</a>.<br/>
        Apple M1: Support using Tensor Cores and simdgroup_float8x8. Some work in <a href="https://github.com/tinygrad/tinygrad/pull/728">this branch</a>.<br/>
        AMD: Write assembly backend for the linearizer.
    </div>
    
    <div>
        <h3>Multi GPU training</h3>
        Trees, rings, and collnets oh my! AMD gets us P2P for free.
    </div>
    
    <div>
        <h3>Kernel Search</h3>
        There's a ton of ways to generate equivalent kernels, enumerate them and search them.<br/>
        Maybe with ML based search running in tinygrad. Self improvement?
    </div>
    
    <div>
        <h3>Kernel Combining (lazy.py)</h3>
        ResNets don't fuse as nicely as they could because which reduce do you fuse the elementwise with?<br/>
        BatchNorm at training and GroupNorm always is 4 kernels. Merge them!
    </div>
    
    <div>
        <img src="tinybox.webp" style="float:right"/>
        <h2>The tinybox</h2>
        <li>738 FP16 TFLOPS</li>
        <li>144 GB GPU RAM</li>
        <li>5.76 TB/s RAM bandwidth</li>
        <li>28.7 GB/s disk read bandwidth (<a href="https://twitter.com/__tinygrad__/status/1747467257889116379">benchmarked</a>)</li>
        <li>AMD EPYC CPU, 32 cores</li>
        <li>2x 1500W (two 120V outlets, can power limit for less)</li>
        <li>Runs 70B FP16 LLaMA-2 out of the box using tinygrad</li>
        <li><b>$15,000</b></li>
        <a href="https://buy.stripe.com/5kAaGL6lk9uX9nW144">Preorder a tinybox today! (just $100)</a>
        (specs subject to change. all preorders fully refundable until your tinybox ships. price doesn't include shipping. estimated timeline 2-6 months)
    </div>
    
    <div>
    <h2>FAQ:</h2>
    
    <em>Is tinygrad used anywhere?</em><br/></br>
    tinygrad is used in <a href="https://github.com/commaai/openpilot">openpilot</a> to run the driving model on the Snapdragon 845 GPU. It replaces <a href="https://developer.qualcomm.com/sites/default/files/docs/snpe/overview.html">SNPE</a>, is faster, supports loading onnx files, supports training, and allows for attention (SNPE only allows fixed weights).
    <br/><br/><br/>
    
    <em>Is tinygrad inference only?</em><br/></br>
    No! It supports full forward and backward passes with autodiff. <a href="https://github.com/tinygrad/tinygrad/blob/master/tinygrad/mlops.py">This</a> is implemented at a level of abstraction higher than the accelerator specific code, so a tinygrad port gets you this for free.
    <br/><br/><br/>
    
    <em>How can I use tinygrad for my next ML project?</em><br/></br>
    Follow the installation instructions on <a href="https://github.com/tinygrad/tinygrad">the tinygrad repo</a>. It has a similar API to PyTorch, yet simpler and more refined. Less stable though while tinygrad is in alpha, so be warned, though it's been fairly stable for a while.

    <em>When will tinygrad leave alpha?</em><br/></br>
    When we can reproduce a common set of papers on 1 NVIDIA GPU 2x faster than PyTorch. We also want the speed to be good on the M1. ETA, Q2 next year.

    <em>How is tinygrad faster than PyTorch?</em><br/></br>
    For most use cases it isn't yet, but it will be. It has three advantages:
    <li>It compiles a custom kernel for every operation, allowing extreme shape specialization.</li>
    <li>All tensors are lazy, so it can aggressively fuse operations.</li>
    <li>The backend is 10x+ simpler, meaning optimizing one kernel makes everything fast.</li>
    <em>How can the tiny corp work for me?</em><br/></br>
    
    Email me, george@tinygrad.org. We are looking for contracts and sponsorships to improve various aspects of tinygrad.
    <em>How can I work for the tiny corp?</em><br/></br>
    
    See <b>hiring</b> above. Contributions to <a href="https://github.com/tinygrad/tinygrad">tinygrad</a> on GitHub always welcome, and a good way to get hired.
    <em>Can I invest in the tiny corp?</em>
    
    Invest with your PRs.

    </div>
</body>
</html>